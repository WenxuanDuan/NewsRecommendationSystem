import os
import re
import nltk
import numpy as np
import gensim.downloader as api
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# **ğŸ“Œ åŠ è½½ Word2Vec é¢„è®­ç»ƒæ¨¡å‹**
print("\nğŸ“Œ Loading Pre-trained Word2Vec Model...\n")
w2v_model = api.load("word2vec-google-news-300")  # 300 ç»´ Google é¢„è®­ç»ƒæ¨¡å‹


# **ğŸ“Œ æ–‡æœ¬é¢„å¤„ç†**
def preprocess_text(text):
    """å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€å»åœç”¨è¯ã€è¯å½¢è¿˜åŸ"""
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words("english"))

    # 1ï¸âƒ£ **è½¬æ¢ä¸ºå°å†™**
    text = text.lower()

    # 2ï¸âƒ£ **ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—**
    text = re.sub(r"[^a-z\s]", "", text)

    # 3ï¸âƒ£ **åˆ†è¯**
    tokens = word_tokenize(text)

    # 4ï¸âƒ£ **å»é™¤åœç”¨è¯ & è¯å½¢è¿˜åŸ**
    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]

    return " ".join(processed_tokens)


# **ğŸ“Œ æ–‡æ¡£è½¬æ¢ä¸º Word2Vec ç‰¹å¾**
def document_to_w2v(doc, model=w2v_model, vector_size=300):
    """å°†æ–‡æ¡£è½¬æ¢ä¸º Word2Vec è¯å‘é‡çš„å‡å€¼"""
    words = doc.split()
    word_vectors = [model[word] for word in words if word in model]

    if word_vectors:
        return np.mean(word_vectors, axis=0)
    else:
        return np.zeros(vector_size)


# **ğŸ“Œ åŠ è½½å¹¶å¤„ç†æ–‡æ¡£**
def load_and_preprocess_documents(data_dir):
    """
    è¯»å–æ•°æ®é›†æ–‡ä»¶å¤¹ï¼Œé¢„å¤„ç†æ–‡æœ¬ï¼Œå¹¶è¿”å› (æ–‡æœ¬åˆ—è¡¨, ç±»åˆ«æ ‡ç­¾åˆ—è¡¨)
    """
    documents, labels = [], []

    for topic in os.listdir(data_dir):
        topic_path = os.path.join(data_dir, topic)

        # **å¿½ç•¥ `.DS_Store` æˆ–éç›®å½•é¡¹**
        if not os.path.isdir(topic_path) or topic.startswith("."):
            continue

        for filename in os.listdir(topic_path):
            file_path = os.path.join(topic_path, filename)

            # **å¿½ç•¥ `.DS_Store` ç­‰éæ–‡æœ¬æ–‡ä»¶**
            if not filename.endswith(".txt"):
                continue

            # **è¯»å–æ–‡ä»¶å†…å®¹**
            with open(file_path, "r", encoding="utf-8", errors="ignore") as file:
                content = file.read().strip()

            # **é¢„å¤„ç†æ–‡æœ¬**
            processed_text = preprocess_text(content)

            if processed_text:  # è¿‡æ»¤ç©ºæ–‡æœ¬
                documents.append(processed_text)
                labels.append(topic)

    return documents, labels

# Example usage:
# doc = Document("001.txt")
# print("Title:", doc.title)
# print("Text:", doc.text)
# print("Title Tokens:", doc.title_tokens)
# print("Text Tokens:", doc.text_tokens)
# print("Document Terms:", doc.document_terms())
#
# print("Word2Vec Vector Shape:", doc.vector.shape)
# print("Word2Vec Vector:", doc.vector)