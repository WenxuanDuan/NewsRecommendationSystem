import os
import json
import pickle
import numpy as np
import random
from document import load_and_preprocess_documents, document_to_w2v, document_to_sbert
from recommendation import load_recommendation_data, build_knn_model, recommend_articles

# âœ… æ§åˆ¶æ˜¯å¦å¯ç”¨åˆ†ç±»è®­ç»ƒ
ENABLE_CLASSIFICATION = False

def prepare_recommendation_data(documents, labels, filenames):
    print("ğŸ“Œ Saving recommendation input files...")
    os.makedirs("data", exist_ok=True)

    # Word2Vec vectors
    vectors = np.array([document_to_w2v(doc) for doc in documents])
    np.save("data/article_vectors.npy", vectors)

    # åŸå§‹æ ‡é¢˜ï¼ˆå¤‡ç”¨ï¼‰
    with open("data/article_titles.json", "w") as f:
        json.dump(filenames, f)

    # ä½¿ç”¨åŸå§‹æ ‡ç­¾
    with open("data/article_labels.json", "w") as f:
        json.dump(labels, f)

    # ä¿å­˜åŸå§‹æ–‡æœ¬
    with open("data/documents.pkl", "wb") as f:
        pickle.dump(documents, f)

    print("âœ… Recommendation input files saved.")


def run_classification(documents, labels):
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.model_selection import KFold
    import torch
    from classification import (
        train_naive_bayes_tfidf,
        train_knn_w2v,
        train_xgboost_tfidf,
        train_xgboost_w2v,
        train_neural_net_w2v,
        train_xgboost_sbert,
        train_neural_net_sbert
    )
    from util import compute_metrics, print_classification_results, plot_cross_validation, plot_confusion_matrix

    print("\nğŸ“Œ Running Classification Evaluation...")
    unique_labels = sorted(set(labels))
    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}
    index_to_label = {idx: label for label, idx in label_to_index.items()}
    y = np.array([label_to_index[label] for label in labels])

    vectorizer = TfidfVectorizer()
    X_tfidf = vectorizer.fit_transform(documents)
    X_w2v = np.array([document_to_w2v(doc) for doc in documents])
    X_sbert = document_to_sbert(documents)

    from run_classifier import run_all_classifiers
    results = run_all_classifiers(X_tfidf, X_w2v, X_sbert, y, unique_labels, index_to_label)

    model_names = {
        "nb": "Naive Bayes + TF-IDF",
        "knn": "kNN + Word2Vec",
        "xgb_tfidf": "XGBoost + TF-IDF",
        "xgb_w2v": "XGBoost + Word2Vec",
        "nn_w2v": "Neural Net + Word2Vec",
        "xgb_sbert": "XGBoost + SBERT",
        "nn_sbert": "Neural Net + SBERT"
    }

    for key, name in model_names.items():
        print(f"\nğŸ“Œ Final Classification Report ({name}):")
        print_classification_results(results[key]["conf_mat"], results[key]["accuracies"], unique_labels)
        plot_cross_validation(results[key]["accuracies"], title=f"{name} Accuracy")
        plot_confusion_matrix(results[key]["conf_mat"], unique_labels, title=f"{name} Confusion Matrix")


def recommend_flow():
    vectors, titles, labels, documents = load_recommendation_data()
    knn_model = build_knn_model(vectors)

    # Step 1: é¦–é¡µæ¨è
    candidate_indices = random.sample(range(len(titles)), 5)
    print("\nğŸ“š ä»¥ä¸‹æ˜¯ä¸ºä½ æ¨èçš„æ–‡ç« ï¼ˆéšæœºæŒ‘é€‰ï¼‰ï¼š\n")
    for i, idx in enumerate(candidate_indices):
        print(f"{i + 1}. [{labels[idx]}] {titles[idx]}")

    choice = input("\nè¯·è¾“å…¥ä½ æƒ³é˜…è¯»çš„æ–‡ç« ç¼–å· (1-5)ï¼Œæˆ–æŒ‰ Q é€€å‡ºç³»ç»Ÿ: ").strip()
    if choice.lower() == "q":
        print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œæ¬¢è¿ä¸‹æ¬¡å†æ¥ï¼")
        return
    selected_index = candidate_indices[int(choice) - 1]
    print(f"\nâœ… ä½ é€‰æ‹©é˜…è¯»ï¼š[{labels[selected_index]}] {titles[selected_index]}\n")
    print("ğŸ“– æ­£æ–‡å†…å®¹å¦‚ä¸‹ï¼š\n")
    print(documents[selected_index])

    # Step 2+: æ— é™æ¨èç›´åˆ°é€€å‡º
    current_index = selected_index
    while True:
        recommended_indices = recommend_articles(knn_model, current_index)
        print("\nğŸ“¢ ä½ å¯èƒ½è¿˜å–œæ¬¢è¿™äº›æ–‡ç« ï¼š\n")
        for i, idx in enumerate(recommended_indices):
            print(f"{i + 1}. [{labels[idx]}] {titles[idx]}")

        next_choice = input("\nè¯·è¾“å…¥ä½ æƒ³ç»§ç»­é˜…è¯»çš„æ–‡ç« ç¼–å· (1-5)ï¼Œæˆ–æŒ‰ Q é€€å‡ºç³»ç»Ÿ: ").strip()
        if next_choice.lower() == "q":
            print("ğŸ‘‹ æ„Ÿè°¢é˜…è¯»ï¼Œå†è§ï¼")
            break
        current_index = recommended_indices[int(next_choice) - 1]
        print(f"\nâœ… ä½ é€‰æ‹©é˜…è¯»ï¼š[{labels[current_index]}] {titles[current_index]}\n")
        print("ğŸ“– æ­£æ–‡å†…å®¹å¦‚ä¸‹ï¼š\n")
        print(documents[current_index])


def main():
    # åŠ è½½æ•°æ®
    print("\nğŸ“Œ Loading and Preprocessing Dataset...")
    data_dir = "../data/bbc/"
    documents, labels, filenames = load_and_preprocess_documents(data_dir)

    # å¯é€‰ï¼šè®­ç»ƒåˆ†ç±»æ¨¡å‹
    if ENABLE_CLASSIFICATION:
        run_classification(documents, labels)

    # ä¿å­˜æ¨èç³»ç»Ÿæ•°æ®ï¼ˆå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼‰
    if not os.path.exists("data/article_vectors.npy"):
        prepare_recommendation_data(documents, labels, filenames)

    # è¿›å…¥æ¨èç³»ç»Ÿäº¤äº’
    recommend_flow()


if __name__ == "__main__":
    main()
